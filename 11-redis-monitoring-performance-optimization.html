<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Redis Monitoring and Performance Optimization: Maximizing Efficiency</title>
</head>
<body>
    <header>
        <h1>Redis Monitoring and Performance Optimization: Maximizing Efficiency</h1>
        <p><em>Master Redis monitoring, performance tuning, and optimization techniques for production environments</em></p>
        <p><strong>Published:</strong> Blog Post #11 of Redis Mastery Series</p>
    </header>

    <main>
        <section>
            <h2>Introduction</h2>
            <p>Monitoring and optimizing Redis performance is crucial for maintaining responsive applications at scale. This guide covers essential monitoring techniques, performance metrics, optimization strategies, and tools for ensuring your Redis deployment runs efficiently.</p>
        </section>

        <section>
            <h2>Essential Performance Metrics</h2>
            
            <h3>Core Redis Metrics</h3>
            <table border="1">
                <tr>
                    <th>Metric</th>
                    <th>Command</th>
                    <th>Good Value</th>
                    <th>Warning Level</th>
                </tr>
                <tr>
                    <td>Memory Usage</td>
                    <td>INFO memory</td>
                    <td>&lt; 70% of maxmemory</td>
                    <td>&gt; 85%</td>
                </tr>
                <tr>
                    <td>CPU Usage</td>
                    <td>INFO cpu</td>
                    <td>&lt; 70%</td>
                    <td>&gt; 85%</td>
                </tr>
                <tr>
                    <td>Connected Clients</td>
                    <td>INFO clients</td>
                    <td>&lt; 80% of maxclients</td>
                    <td>&gt; 90%</td>
                </tr>
                <tr>
                    <td>Commands/sec</td>
                    <td>INFO stats</td>
                    <td>Varies by use case</td>
                    <td>Sudden drops</td>
                </tr>
                <tr>
                    <td>Keyspace Hits Ratio</td>
                    <td>INFO stats</td>
                    <td>&gt; 90%</td>
                    <td>&lt; 80%</td>
                </tr>
            </table>

            <h3>Memory Analysis</h3>
            <pre><code># Memory information
INFO memory

# Key memory metrics:
used_memory_human:2.50M
used_memory_rss_human:8.25M
used_memory_peak_human:7.77M
mem_fragmentation_ratio:3.30
mem_allocator:jemalloc-5.1.0

# Memory usage by key
MEMORY USAGE key_name

# Find largest keys
redis-cli --bigkeys

# Memory doctor recommendations
MEMORY DOCTOR</code></pre>

            <h3>Performance Statistics</h3>
            <pre><code># Overall statistics
INFO stats

# Key performance metrics:
total_commands_processed:12345
instantaneous_ops_per_sec:150
keyspace_hits:8500
keyspace_misses:1500
expired_keys:234
evicted_keys:0

# Command statistics
INFO commandstats
# Returns usage stats for each command type</code></pre>
        </section>

        <section>
            <h2>Monitoring Tools and Setup</h2>
            
            <h3>Redis Exporter for Prometheus</h3>
            <pre><code># docker-compose.yml for monitoring stack
version: '3.8'
services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
      
  redis-exporter:
    image: oliver006/redis_exporter
    ports:
      - "9121:9121"
    environment:
      REDIS_ADDR: redis://redis:6379
      
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"</code></pre>

            <h3>Prometheus Configuration</h3>
            <pre><code># prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    scrape_interval: 10s</code></pre>

            <h3>Custom Monitoring Script</h3>
            <pre><code>#!/bin/bash
# redis_monitor.sh
REDIS_HOST="localhost"
REDIS_PORT="6379"
THRESHOLD_MEMORY=80
THRESHOLD_CPU=75

# Function to get Redis info
get_redis_info() {
    redis-cli -h $REDIS_HOST -p $REDIS_PORT INFO $1
}

# Check memory usage
check_memory() {
    local memory_info=$(get_redis_info memory)
    local used_memory=$(echo "$memory_info" | grep "used_memory:" | cut -d: -f2)
    local maxmemory=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT CONFIG GET maxmemory | tail -1)
    
    if [ "$maxmemory" != "0" ]; then
        local usage_percent=$((used_memory * 100 / maxmemory))
        echo "Memory usage: ${usage_percent}%"
        
        if [ $usage_percent -gt $THRESHOLD_MEMORY ]; then
            echo "WARNING: High memory usage!"
        fi
    fi
}

# Check connected clients
check_clients() {
    local clients_info=$(get_redis_info clients)
    local connected=$(echo "$clients_info" | grep "connected_clients:" | cut -d: -f2)
    local maxclients=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT CONFIG GET maxclients | tail -1)
    
    echo "Connected clients: $connected (max: $maxclients)"
    
    local usage_percent=$((connected * 100 / maxclients))
    if [ $usage_percent -gt 80 ]; then
        echo "WARNING: High client connection usage!"
    fi
}

# Check slow queries
check_slow_queries() {
    echo "Recent slow queries:"
    redis-cli -h $REDIS_HOST -p $REDIS_PORT SLOWLOG GET 5
}

# Main monitoring function
main() {
    echo "=== Redis Health Check $(date) ==="
    check_memory
    check_clients
    check_slow_queries
    echo "=================================="
}

main</code></pre>
        </section>

        <section>
            <h2>Performance Optimization Techniques</h2>
            
            <h3>Configuration Optimization</h3>
            <pre><code># redis.conf optimization for performance

# Memory management
maxmemory 4gb
maxmemory-policy allkeys-lru
maxmemory-samples 10

# Network optimization
tcp-nodelay yes
tcp-keepalive 60
timeout 300

# Persistence optimization
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error no

# AOF optimization
appendonly yes
appendfsync everysec
no-appendfsync-on-rewrite yes
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# Client connections
maxclients 10000
tcp-backlog 511

# Performance tuning
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64</code></pre>

            <h3>Memory Optimization</h3>
            <pre><code># Check memory fragmentation
INFO memory | grep fragmentation

# If fragmentation > 1.5, consider:
# 1. Redis restart (nuclear option)
# 2. MEMORY PURGE (Redis 4.0+)
MEMORY PURGE

# 3. Enable active defragmentation (Redis 4.0+)
CONFIG SET activedefrag yes
CONFIG SET active-defrag-ignore-bytes 100mb
CONFIG SET active-defrag-threshold-lower 10
CONFIG SET active-defrag-threshold-upper 100</code></pre>

            <h3>Key Design Optimization</h3>
            <pre><code># Good: Use appropriate data structures
# For small objects, use hashes instead of multiple strings
HSET user:1000 name "John" email "john@example.com" age 30

# Bad: Multiple keys for same object
SET user:1000:name "John"
SET user:1000:email "john@example.com"
SET user:1000:age 30

# Good: Use expiration to prevent memory leaks
SET session:abc123 "user_data" EX 3600

# Good: Use appropriate key naming patterns
user:1000:profile
order:12345:items
cache:weather:london:2023-12-01</code></pre>

            <h3>Command Optimization</h3>
            <pre><code># Use pipeline for multiple operations
import redis

# Efficient: Pipeline
pipe = redis_client.pipeline()
for i in range(1000):
    pipe.set(f"key:{i}", f"value:{i}")
pipe.execute()

# Inefficient: Individual commands
for i in range(1000):
    redis_client.set(f"key:{i}", f"value:{i}")

# Use MGET/MSET for multiple keys
MSET key1 value1 key2 value2 key3 value3
MGET key1 key2 key3

# Use Lua scripts for complex atomic operations
EVAL "
for i=1,#KEYS do
    redis.call('INCR', KEYS[i])
end
return #KEYS
" 3 counter1 counter2 counter3</code></pre>
        </section>

        <section>
            <h2>Performance Testing and Benchmarking</h2>
            
            <h3>Redis Benchmark Tool</h3>
            <pre><code># Basic benchmark
redis-benchmark

# Specific operations
redis-benchmark -t set,get -n 100000

# Custom data size
redis-benchmark -t set -n 100000 -d 1000

# Pipeline benchmark
redis-benchmark -t set,get -n 100000 -P 16

# Quiet mode (summary only)
redis-benchmark -t set,get -n 100000 -q

# Specific host and port
redis-benchmark -h redis.example.com -p 6379 -n 50000</code></pre>

            <h3>Custom Performance Tests</h3>
            <pre><code>import redis
import time
import statistics
from concurrent.futures import ThreadPoolExecutor

class RedisBenchmark:
    def __init__(self, host='localhost', port=6379):
        self.redis_client = redis.Redis(host=host, port=port, decode_responses=True)
    
    def benchmark_operations(self, operation_func, iterations=1000, threads=1):
        """Benchmark Redis operations with threading"""
        
        def run_operations(thread_id):
            thread_times = []
            for i in range(iterations // threads):
                start_time = time.time()
                try:
                    operation_func(f"{thread_id}:{i}")
                    success = True
                except Exception as e:
                    success = False
                end_time = time.time()
                thread_times.append((end_time - start_time, success))
            return thread_times
        
        # Run benchmark with multiple threads
        with ThreadPoolExecutor(max_workers=threads) as executor:
            futures = [executor.submit(run_operations, i) for i in range(threads)]
            all_times = []
            all_successes = []
            
            for future in futures:
                thread_results = future.result()
                for duration, success in thread_results:
                    all_times.append(duration)
                    all_successes.append(success)
        
        # Calculate statistics
        success_rate = sum(all_successes) / len(all_successes)
        avg_time = statistics.mean(all_times)
        median_time = statistics.median(all_times)
        p95_time = statistics.quantiles(all_times, n=20)[18]  # 95th percentile
        ops_per_second = 1 / avg_time if avg_time > 0 else 0
        
        return {
            'avg_time': avg_time,
            'median_time': median_time,
            'p95_time': p95_time,
            'ops_per_second': ops_per_second,
            'success_rate': success_rate
        }
    
    def test_set_operations(self, key_suffix):
        """Test SET operations"""
        self.redis_client.set(f"benchmark:set:{key_suffix}", f"value_{key_suffix}")
    
    def test_get_operations(self, key_suffix):
        """Test GET operations"""
        self.redis_client.get(f"benchmark:get:{key_suffix}")
    
    def test_hash_operations(self, key_suffix):
        """Test HSET operations"""
        self.redis_client.hset(f"benchmark:hash:{key_suffix}", "field", f"value_{key_suffix}")
    
    def test_list_operations(self, key_suffix):
        """Test LPUSH operations"""
        self.redis_client.lpush(f"benchmark:list:{key_suffix}", f"value_{key_suffix}")
    
    def run_comprehensive_benchmark(self):
        """Run comprehensive performance benchmark"""
        tests = {
            'SET': self.test_set_operations,
            'GET': self.test_get_operations,
            'HSET': self.test_hash_operations,
            'LPUSH': self.test_list_operations
        }
        
        results = {}
        for test_name, test_func in tests.items():
            print(f"Running {test_name} benchmark...")
            results[test_name] = self.benchmark_operations(test_func, iterations=10000, threads=4)
        
        return results

# Usage
benchmark = RedisBenchmark()
results = benchmark.run_comprehensive_benchmark()

for test_name, metrics in results.items():
    print(f"\n{test_name} Results:")
    print(f"  Ops/sec: {metrics['ops_per_second']:.0f}")
    print(f"  Avg time: {metrics['avg_time']*1000:.2f}ms")
    print(f"  P95 time: {metrics['p95_time']*1000:.2f}ms")
    print(f"  Success rate: {metrics['success_rate']:.2%}")</code></pre>
        </section>

        <section>
            <h2>Troubleshooting Performance Issues</h2>
            
            <h3>Common Performance Problems</h3>
            
            <h4>High Memory Usage</h4>
            <pre><code># Diagnose memory issues
MEMORY DOCTOR
redis-cli --bigkeys
redis-cli --memkeys

# Solutions:
# 1. Set appropriate TTL
EXPIRE large_key 3600

# 2. Use more efficient data structures
# Replace large strings with hashes for structured data

# 3. Enable LRU eviction
CONFIG SET maxmemory-policy allkeys-lru

# 4. Optimize data encoding
CONFIG SET hash-max-ziplist-entries 512</code></pre>

            <h4>Slow Queries</h4>
            <pre><code># Check slow log
SLOWLOG GET 10
SLOWLOG LEN
SLOWLOG RESET

# Configure slow log threshold
CONFIG SET slowlog-log-slower-than 10000  # 10ms

# Optimize slow operations:
# 1. Avoid KEYS in production
# Use SCAN instead
SCAN 0 MATCH pattern* COUNT 100

# 2. Limit expensive operations
LRANGE mylist 0 99  # Instead of 0 -1
ZRANGE myzset 0 99  # Limit range operations</code></pre>

            <h4>High CPU Usage</h4>
            <pre><code># Check CPU usage
INFO cpu

# Common causes and solutions:
# 1. Complex operations - use Lua scripts
# 2. Many small operations - use pipelining
# 3. Inappropriate data structures - optimize choices
# 4. Too many clients - implement connection pooling</code></pre>

            <h3>Performance Monitoring Dashboard</h3>
            <pre><code>import redis
import time
import json
from datetime import datetime

class RedisMonitoringDashboard:
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
    
    def collect_metrics(self):
        """Collect comprehensive Redis metrics"""
        info_sections = ['memory', 'stats', 'clients', 'cpu', 'persistence']
        metrics = {'timestamp': datetime.now().isoformat()}
        
        for section in info_sections:
            try:
                section_info = self.redis_client.info(section)
                metrics[section] = section_info
            except Exception as e:
                metrics[section] = {'error': str(e)}
        
        # Additional metrics
        try:
            metrics['slowlog_len'] = self.redis_client.slowlog_len()
            metrics['dbsize'] = self.redis_client.dbsize()
            metrics['config'] = {
                'maxmemory': self.redis_client.config_get('maxmemory')['maxmemory'],
                'maxclients': self.redis_client.config_get('maxclients')['maxclients']
            }
        except Exception as e:
            metrics['additional_error'] = str(e)
        
        return metrics
    
    def calculate_derived_metrics(self, metrics):
        """Calculate derived performance metrics"""
        if 'memory' in metrics and 'stats' in metrics:
            memory = metrics['memory']
            stats = metrics['stats']
            
            # Hit ratio
            hits = stats.get('keyspace_hits', 0)
            misses = stats.get('keyspace_misses', 0)
            total_requests = hits + misses
            hit_ratio = (hits / total_requests * 100) if total_requests > 0 else 0
            
            # Memory efficiency
            used_memory = memory.get('used_memory', 0)
            maxmemory = int(metrics.get('config', {}).get('maxmemory', 0))
            memory_usage_pct = (used_memory / maxmemory * 100) if maxmemory > 0 else 0
            
            # Fragmentation
            fragmentation = memory.get('mem_fragmentation_ratio', 1.0)
            
            derived = {
                'hit_ratio_pct': hit_ratio,
                'memory_usage_pct': memory_usage_pct,
                'fragmentation_ratio': fragmentation,
                'ops_per_sec': stats.get('instantaneous_ops_per_sec', 0)
            }
            
            return derived
        
        return {}
    
    def generate_health_report(self):
        """Generate health status report"""
        metrics = self.collect_metrics()
        derived = self.calculate_derived_metrics(metrics)
        
        health_status = 'HEALTHY'
        issues = []
        
        # Check various health indicators
        if derived.get('memory_usage_pct', 0) > 85:
            health_status = 'WARNING'
            issues.append(f"High memory usage: {derived['memory_usage_pct']:.1f}%")
        
        if derived.get('hit_ratio_pct', 100) < 80:
            health_status = 'WARNING'
            issues.append(f"Low hit ratio: {derived['hit_ratio_pct']:.1f}%")
        
        if derived.get('fragmentation_ratio', 1.0) > 1.5:
            health_status = 'WARNING'
            issues.append(f"High fragmentation: {derived['fragmentation_ratio']:.2f}")
        
        if metrics.get('slowlog_len', 0) > 100:
            health_status = 'WARNING'
            issues.append(f"Many slow queries: {metrics['slowlog_len']}")
        
        return {
            'status': health_status,
            'issues': issues,
            'metrics': derived,
            'timestamp': metrics['timestamp']
        }

# Usage
monitor = RedisMonitoringDashboard()

# Continuous monitoring
while True:
    report = monitor.generate_health_report()
    print(f"Status: {report['status']}")
    if report['issues']:
        print("Issues:", ', '.join(report['issues']))
    print(f"Metrics: {report['metrics']}")
    print("-" * 50)
    time.sleep(60)  # Check every minute</code></pre>
        </section>

        <section>
            <h2>Best Practices Summary</h2>
            
            <h3>Monitoring Best Practices</h3>
            <ul>
                <li>Monitor key metrics: memory, CPU, connections, hit ratio</li>
                <li>Set up alerting for critical thresholds</li>
                <li>Use proper monitoring tools (Prometheus + Grafana)</li>
                <li>Regularly review slow query logs</li>
                <li>Track performance trends over time</li>
                <li>Monitor replication lag in clustered setups</li>
            </ul>

            <h3>Optimization Best Practices</h3>
            <ul>
                <li>Choose appropriate data structures for use cases</li>
                <li>Use pipelining for batch operations</li>
                <li>Set appropriate TTL values</li>
                <li>Optimize memory settings and policies</li>
                <li>Avoid expensive operations in production</li>
                <li>Use connection pooling effectively</li>
                <li>Regular performance testing and benchmarking</li>
            </ul>
        </section>

        <section>
            <h2>Key Takeaways</h2>
            <ul>
                <li>Continuous monitoring is essential for production Redis</li>
                <li>Memory usage and fragmentation are critical metrics</li>
                <li>Proper configuration significantly impacts performance</li>
                <li>Use appropriate tools for monitoring and alerting</li>
                <li>Regular performance testing helps identify issues early</li>
                <li>Optimization should be based on actual metrics, not assumptions</li>
                <li>Prevention is better than reactive performance fixes</li>
            </ul>
        </section>
    </main>

    <footer>
        <hr>
        <p><strong>Previous:</strong> <a href="10-extending-redis-with-modules.html">Extending Redis with Modules</a></p>
        <p><strong>Next up:</strong> Redis Security Best Practices</p>
    </footer>
</body>
</html>
