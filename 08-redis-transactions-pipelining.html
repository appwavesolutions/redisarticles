<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Redis Transactions and Pipelining: Ensuring Data Consistency and Performance</title>
</head>
<body>
    <header>
        <h1>Redis Transactions and Pipelining: Ensuring Data Consistency and Performance</h1>
        <p><em>Master atomic operations, optimistic concurrency control, and performance optimization techniques</em></p>
        <p><strong>Published:</strong> Blog Post #8 of Redis Mastery Series</p>
    </header>

    <main>
        <section>
            <h2>Introduction</h2>
            <p>In multi-client environments, ensuring data consistency while maintaining high performance is crucial. Redis provides powerful mechanisms for both: transactions ensure that multiple operations execute atomically, while pipelining dramatically improves performance by reducing network round-trips.</p>
            
            <p>Understanding these concepts is essential for building robust, high-performance applications. Whether you're implementing financial transactions that must be atomic, or optimizing performance for high-throughput scenarios, this guide will provide you with the knowledge and practical examples needed to master Redis transactions and pipelining.</p>
            
            <p>We'll explore MULTI/EXEC transactions, optimistic concurrency control with WATCH, advanced pipelining techniques, and real-world patterns that combine both for maximum effectiveness.</p>
        </section>

        <section>
            <h2>Understanding Redis Transactions</h2>
            
            <h3>Transaction Fundamentals</h3>
            <p>Redis transactions provide a way to execute multiple commands as a single atomic operation. Key characteristics:</p>
            <ul>
                <li><strong>Atomicity:</strong> All commands execute or none do</li>
                <li><strong>Isolation:</strong> No other client commands can execute during the transaction</li>
                <li><strong>Queuing:</strong> Commands are queued and executed together</li>
                <li><strong>No rollback:</strong> Failed commands don't rollback successful ones</li>
            </ul>

            <h3>Basic Transaction Commands</h3>
            
            <h4>MULTI/EXEC Pattern</h4>
            <pre><code># Start transaction
MULTI
OK

# Queue commands (they don't execute yet)
SET user:1000:balance 100
QUEUED
INCR user:1000:login_count  
QUEUED
SADD user:1000:permissions "read" "write"
QUEUED

# Execute all commands atomically
EXEC
1) OK
2) (integer) 1
3) (integer) 2

# Alternative: Discard transaction
MULTI
SET temp:key "value"
INCR temp:counter
DISCARD  # All queued commands are discarded</code></pre>

            <h4>Transaction Behavior</h4>
            <pre><code># Commands are queued, not executed immediately
MULTI
GET user:1000:balance
SET user:1000:balance 200
GET user:1000:balance  # This won't see the SET result yet
EXEC
# Results:
1) "100"     # Original balance
2) OK        # SET result  
3) "200"     # New balance

# Syntax errors cause transaction to be discarded
MULTI
SET key1 value1
INVALID_COMMAND  # Syntax error
EXEC
# (error) EXECABORT Transaction discarded because of previous errors</code></pre>

            <h3>Error Handling in Transactions</h3>
            
            <h4>Runtime vs Syntax Errors</h4>
            <pre><code># Syntax errors: Transaction is discarded
MULTI
SET key1 value1
WRONGCOMMAND key2 value2  # Invalid command syntax
EXEC
# (error) EXECABORT Transaction discarded

# Runtime errors: Transaction continues
MULTI
SET key1 value1
INCR key1  # Runtime error: key1 is not a number
SET key2 value2
EXEC
# Results:
1) OK
2) (error) ERR value is not an integer or out of range
3) OK
# key1 = "value1", key2 = "value2" (partial execution)</code></pre>

            <h3>Practical Transaction Examples</h3>
            
            <h4>Bank Transfer Implementation</h4>
            <pre><code>import redis
import time
from typing import Optional

class BankingService:
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
    
    def transfer_funds(self, from_account: str, to_account: str, amount: float) -> bool:
        """Transfer funds between accounts atomically"""
        
        # Keys for account balances
        from_key = f"account:{from_account}:balance"
        to_key = f"account:{to_account}:balance"
        
        try:
            # Start transaction
            pipe = self.redis_client.pipeline()
            pipe.multi()
            
            # Get current balances first (outside transaction)
            from_balance = self.redis_client.get(from_key)
            to_balance = self.redis_client.get(to_key)
            
            # Validate balances exist
            if from_balance is None or to_balance is None:
                return False
            
            from_balance = float(from_balance)
            to_balance = float(to_balance)
            
            # Check sufficient funds
            if from_balance < amount:
                return False
            
            # Perform transfer
            pipe.set(from_key, from_balance - amount)
            pipe.set(to_key, to_balance + amount)
            
            # Log transaction
            transaction_id = f"tx_{int(time.time() * 1000)}"
            pipe.lpush("transactions:log", f"{transaction_id}:{from_account}:{to_account}:{amount}:{time.time()}")
            
            # Execute transaction
            results = pipe.execute()
            
            # Check if all operations succeeded
            return all(result == 'OK' or isinstance(result, int) for result in results)
            
        except redis.RedisError as e:
            print(f"Transaction failed: {e}")
            return False
    
    def get_balance(self, account: str) -> Optional[float]:
        """Get account balance"""
        balance = self.redis_client.get(f"account:{account}:balance")
        return float(balance) if balance else None
    
    def create_account(self, account: str, initial_balance: float = 0.0) -> bool:
        """Create new account"""
        key = f"account:{account}:balance"
        # Use SETNX to prevent overwriting existing accounts
        return self.redis_client.setnx(key, initial_balance)

# Usage example
banking = BankingService()

# Create accounts
banking.create_account("alice", 1000.0)
banking.create_account("bob", 500.0)

# Transfer funds
success = banking.transfer_funds("alice", "bob", 250.0)
print(f"Transfer successful: {success}")
print(f"Alice balance: {banking.get_balance('alice')}")  # 750.0
print(f"Bob balance: {banking.get_balance('bob')}")      # 750.0</code></pre>

            <h4>Shopping Cart Management</h4>
            <pre><code>import redis
import json
from typing import Dict, List, Optional

class ShoppingCartService:
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
    
    def add_to_cart(self, user_id: str, product_id: str, quantity: int, price: float) -> bool:
        """Add item to cart atomically"""
        cart_key = f"cart:{user_id}"
        cart_total_key = f"cart:{user_id}:total"
        
        try:
            pipe = self.redis_client.pipeline()
            pipe.multi()
            
            # Add/update item in cart (using hash)
            pipe.hset(cart_key, product_id, json.dumps({
                'quantity': quantity,
                'price': price,
                'added_at': time.time()
            }))
            
            # Update cart total
            current_total = self.redis_client.get(cart_total_key) or "0"
            new_total = float(current_total) + (quantity * price)
            pipe.set(cart_total_key, new_total)
            
            # Set cart expiration (30 days)
            pipe.expire(cart_key, 86400 * 30)
            pipe.expire(cart_total_key, 86400 * 30)
            
            # Update last activity
            pipe.set(f"cart:{user_id}:last_activity", time.time())
            
            results = pipe.execute()
            return all(results)
            
        except redis.RedisError as e:
            print(f"Failed to add to cart: {e}")
            return False
    
    def remove_from_cart(self, user_id: str, product_id: str) -> bool:
        """Remove item from cart atomically"""
        cart_key = f"cart:{user_id}"
        cart_total_key = f"cart:{user_id}:total"
        
        try:
            # Get item details before removal
            item_data = self.redis_client.hget(cart_key, product_id)
            if not item_data:
                return False
            
            item = json.loads(item_data)
            item_total = item['quantity'] * item['price']
            
            pipe = self.redis_client.pipeline()
            pipe.multi()
            
            # Remove item
            pipe.hdel(cart_key, product_id)
            
            # Update total
            current_total = self.redis_client.get(cart_total_key) or "0"
            new_total = max(0, float(current_total) - item_total)
            pipe.set(cart_total_key, new_total)
            
            # Update last activity
            pipe.set(f"cart:{user_id}:last_activity", time.time())
            
            results = pipe.execute()
            return all(results)
            
        except redis.RedisError as e:
            print(f"Failed to remove from cart: {e}")
            return False
    
    def checkout_cart(self, user_id: str) -> Optional[Dict]:
        """Checkout cart and clear it atomically"""
        cart_key = f"cart:{user_id}"
        cart_total_key = f"cart:{user_id}:total"
        
        try:
            # Get cart contents
            cart_items = self.redis_client.hgetall(cart_key)
            cart_total = self.redis_client.get(cart_total_key)
            
            if not cart_items:
                return None
            
            # Create order
            order_id = f"order_{int(time.time() * 1000)}"
            order_data = {
                'order_id': order_id,
                'user_id': user_id,
                'items': cart_items,
                'total': float(cart_total) if cart_total else 0.0,
                'created_at': time.time()
            }
            
            pipe = self.redis_client.pipeline()
            pipe.multi()
            
            # Save order
            pipe.set(f"order:{order_id}", json.dumps(order_data))
            
            # Clear cart
            pipe.delete(cart_key, cart_total_key, f"cart:{user_id}:last_activity")
            
            # Add to user's order history
            pipe.lpush(f"user:{user_id}:orders", order_id)
            
            results = pipe.execute()
            
            if all(results):
                return order_data
            else:
                return None
                
        except redis.RedisError as e:
            print(f"Checkout failed: {e}")
            return None</code></pre>
        </section>

        <section>
            <h2>Optimistic Concurrency with WATCH</h2>
            
            <h3>Understanding WATCH/UNWATCH</h3>
            <p>WATCH provides optimistic concurrency control by monitoring keys for changes:</p>
            <ul>
                <li><strong>WATCH key:</strong> Monitor key for changes</li>
                <li><strong>Transaction fails:</strong> If watched key changes before EXEC</li>
                <li><strong>UNWATCH:</strong> Stop watching all keys</li>
                <li><strong>Automatic cleanup:</strong> WATCH is cleared after EXEC/DISCARD</li>
            </ul>

            <h3>WATCH Examples</h3>
            
            <h4>Safe Counter Increment</h4>
            <pre><code># Client 1
WATCH counter
counter_value = GET counter  # Returns "10"
MULTI
SET counter 11  # counter_value + 1
EXEC

# If another client modifies 'counter' between WATCH and EXEC,
# the transaction returns (nil) indicating failure</code></pre>

            <h4>Compare-and-Swap Implementation</h4>
            <pre><code>import redis
import time
import random

class AtomicOperations:
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
    
    def atomic_increment(self, key: str, increment: int = 1, max_retries: int = 100) -> Optional[int]:
        """Atomically increment a value with retry on conflict"""
        
        for attempt in range(max_retries):
            try:
                # Watch the key
                self.redis_client.watch(key)
                
                # Get current value
                current_value = self.redis_client.get(key)
                if current_value is None:
                    current_value = 0
                else:
                    current_value = int(current_value)
                
                # Calculate new value
                new_value = current_value + increment
                
                # Start transaction
                pipe = self.redis_client.pipeline()
                pipe.multi()
                pipe.set(key, new_value)
                
                # Execute transaction
                result = pipe.execute()
                
                # Check if transaction succeeded
                if result is not None:
                    return new_value
                
                # Transaction failed due to key modification
                # Wait briefly before retry
                time.sleep(0.001 * random.randint(1, 10))
                
            except redis.WatchError:
                # Key was modified, retry
                continue
            except Exception as e:
                print(f"Error in atomic increment: {e}")
                break
        
        # Max retries exceeded
        return None
    
    def compare_and_swap(self, key: str, expected_value: str, new_value: str) -> bool:
        """Compare and swap operation"""
        try:
            self.redis_client.watch(key)
            
            current_value = self.redis_client.get(key)
            
            # Check if current value matches expected
            if current_value != expected_value:
                self.redis_client.unwatch()
                return False
            
            # Perform swap
            pipe = self.redis_client.pipeline()
            pipe.multi()
            pipe.set(key, new_value)
            
            result = pipe.execute()
            return result is not None
            
        except redis.WatchError:
            return False
    
    def atomic_list_append_unique(self, list_key: str, value: str) -> bool:
        """Append to list only if value doesn't already exist"""
        try:
            self.redis_client.watch(list_key)
            
            # Check if value already exists
            existing_values = self.redis_client.lrange(list_key, 0, -1)
            if value in existing_values:
                self.redis_client.unwatch()
                return False
            
            # Append value
            pipe = self.redis_client.pipeline()
            pipe.multi()
            pipe.lpush(list_key, value)
            
            result = pipe.execute()
            return result is not None
            
        except redis.WatchError:
            return False
    
    def atomic_counter_with_limit(self, key: str, limit: int) -> Optional[int]:
        """Increment counter only if it doesn't exceed limit"""
        try:
            self.redis_client.watch(key)
            
            current_value = self.redis_client.get(key)
            current_value = int(current_value) if current_value else 0
            
            # Check limit
            if current_value >= limit:
                self.redis_client.unwatch()
                return None
            
            new_value = current_value + 1
            
            pipe = self.redis_client.pipeline()
            pipe.multi()
            pipe.set(key, new_value)
            
            result = pipe.execute()
            return new_value if result is not None else None
            
        except redis.WatchError:
            return None

# Usage examples
atomic_ops = AtomicOperations()

# Safe increment with retries
new_value = atomic_ops.atomic_increment("page_views", 1)
print(f"New page view count: {new_value}")

# Compare and swap
success = atomic_ops.compare_and_swap("status", "pending", "processing")
print(f"Status updated: {success}")

# Counter with limit (rate limiting)
result = atomic_ops.atomic_counter_with_limit("api_calls:user123", 1000)
if result:
    print(f"API call allowed, count: {result}")
else:
    print("API call limit exceeded")</code></pre>

            <h3>Distributed Locking with WATCH</h3>
            
            <h4>Simple Distributed Lock</h4>
            <pre><code>import redis
import time
import uuid
from contextlib import contextmanager

class DistributedLock:
    def __init__(self, redis_client, lock_key: str, timeout: int = 10):
        self.redis_client = redis_client
        self.lock_key = f"lock:{lock_key}"
        self.timeout = timeout
        self.lock_value = str(uuid.uuid4())
        self.acquired = False
    
    def acquire(self, blocking: bool = True, blocking_timeout: int = None) -> bool:
        """Acquire distributed lock"""
        deadline = time.time() + (blocking_timeout or 0) if blocking_timeout else None
        
        while True:
            try:
                # Try to acquire lock with WATCH for atomic check-and-set
                self.redis_client.watch(self.lock_key)
                
                # Check if lock is available
                current_lock = self.redis_client.get(self.lock_key)
                
                if current_lock is None:
                    # Lock is available, try to acquire
                    pipe = self.redis_client.pipeline()
                    pipe.multi()
                    pipe.set(self.lock_key, self.lock_value, ex=self.timeout)
                    
                    result = pipe.execute()
                    
                    if result is not None:
                        self.acquired = True
                        return True
                else:
                    # Lock is held, check if it's expired
                    lock_ttl = self.redis_client.ttl(self.lock_key)
                    if lock_ttl == -1:  # No expiration set, force expire
                        self.redis_client.expire(self.lock_key, self.timeout)
                
                self.redis_client.unwatch()
                
                if not blocking:
                    return False
                
                if deadline and time.time() > deadline:
                    return False
                
                # Wait before retry
                time.sleep(0.01)
                
            except redis.WatchError:
                # Key was modified, retry
                continue
    
    def release(self) -> bool:
        """Release distributed lock"""
        if not self.acquired:
            return False
        
        try:
            # Use Lua script for atomic check-and-delete
            lua_script = """
            if redis.call("GET", KEYS[1]) == ARGV[1] then
                return redis.call("DEL", KEYS[1])
            else
                return 0
            end
            """
            
            result = self.redis_client.eval(lua_script, 1, self.lock_key, self.lock_value)
            self.acquired = False
            return result == 1
            
        except Exception as e:
            print(f"Error releasing lock: {e}")
            return False
    
    @contextmanager
    def __call__(self, blocking: bool = True, blocking_timeout: int = None):
        """Context manager for automatic lock acquisition/release"""
        acquired = self.acquire(blocking, blocking_timeout)
        
        if not acquired:
            raise Exception(f"Could not acquire lock: {self.lock_key}")
        
        try:
            yield
        finally:
            self.release()

# Usage example
def critical_section_example():
    redis_client = redis.Redis(decode_responses=True)
    
    # Using context manager
    with DistributedLock(redis_client, "critical_resource", timeout=30):
        print("Performing critical operation...")
        # Only one process can execute this code at a time
        time.sleep(5)
        print("Critical operation completed")
    
    # Manual lock management
    lock = DistributedLock(redis_client, "another_resource")
    if lock.acquire(blocking=True, blocking_timeout=5):
        try:
            print("Lock acquired, doing work...")
            time.sleep(2)
        finally:
            lock.release()
    else:
        print("Could not acquire lock within timeout")</code></pre>
        </section>

        <section>
            <h2>Redis Pipelining</h2>
            
            <h3>Understanding Pipelining</h3>
            <p>Pipelining allows sending multiple commands without waiting for individual responses:</p>
            <ul>
                <li><strong>Reduced latency:</strong> Fewer network round-trips</li>
                <li><strong>Higher throughput:</strong> Batch command execution</li>
                <li><strong>Order preservation:</strong> Responses return in command order</li>
                <li><strong>No atomicity:</strong> Commands execute independently</li>
            </ul>

            <h3>Basic Pipelining Examples</h3>
            
            <h4>Performance Comparison</h4>
            <pre><code>import redis
import time

def without_pipelining(redis_client, n=1000):
    """Execute commands without pipelining"""
    start_time = time.time()
    
    for i in range(n):
        redis_client.set(f"key:{i}", f"value:{i}")
        redis_client.get(f"key:{i}")
    
    end_time = time.time()
    return end_time - start_time

def with_pipelining(redis_client, n=1000):
    """Execute commands with pipelining"""
    start_time = time.time()
    
    pipe = redis_client.pipeline()
    
    for i in range(n):
        pipe.set(f"key:{i}", f"value:{i}")
        pipe.get(f"key:{i}")
    
    results = pipe.execute()
    
    end_time = time.time()
    return end_time - start_time

# Performance test
redis_client = redis.Redis(decode_responses=True)

time_without = without_pipelining(redis_client, 1000)
time_with = with_pipelining(redis_client, 1000)

print(f"Without pipelining: {time_without:.3f}s")
print(f"With pipelining: {time_with:.3f}s")
print(f"Speedup: {time_without/time_with:.1f}x")</code></pre>

            <h4>Batch Operations</h4>
            <pre><code>import redis
from typing import Dict, List

class BatchOperations:
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
    
    def batch_set(self, key_value_pairs: Dict[str, str]) -> List[bool]:
        """Set multiple key-value pairs efficiently"""
        pipe = self.redis_client.pipeline()
        
        for key, value in key_value_pairs.items():
            pipe.set(key, value)
        
        results = pipe.execute()
        return [result == 'OK' for result in results]
    
    def batch_get(self, keys: List[str]) -> Dict[str, str]:
        """Get multiple keys efficiently"""
        pipe = self.redis_client.pipeline()
        
        for key in keys:
            pipe.get(key)
        
        results = pipe.execute()
        return dict(zip(keys, results))
    
    def batch_increment(self, keys: List[str]) -> Dict[str, int]:
        """Increment multiple counters"""
        pipe = self.redis_client.pipeline()
        
        for key in keys:
            pipe.incr(key)
        
        results = pipe.execute()
        return dict(zip(keys, results))
    
    def batch_list_operations(self, operations: List[tuple]) -> List:
        """Perform multiple list operations
        operations: List of tuples (operation, key, *args)
        """
        pipe = self.redis_client.pipeline()
        
        for operation in operations:
            op_type, key, *args = operation
            
            if op_type == 'lpush':
                pipe.lpush(key, *args)
            elif op_type == 'rpush':
                pipe.rpush(key, *args)
            elif op_type == 'lpop':
                pipe.lpop(key)
            elif op_type == 'rpop':
                pipe.rpop(key)
            elif op_type == 'lrange':
                start, stop = args
                pipe.lrange(key, start, stop)
        
        return pipe.execute()
    
    def batch_hash_operations(self, hash_key: str, field_operations: List[tuple]) -> List:
        """Perform multiple hash operations on single key"""
        pipe = self.redis_client.pipeline()
        
        for operation in field_operations:
            op_type, field, *args = operation
            
            if op_type == 'hset':
                value = args[0]
                pipe.hset(hash_key, field, value)
            elif op_type == 'hget':
                pipe.hget(hash_key, field)
            elif op_type == 'hdel':
                pipe.hdel(hash_key, field)
            elif op_type == 'hincrby':
                increment = args[0]
                pipe.hincrby(hash_key, field, increment)
        
        return pipe.execute()

# Usage examples
batch_ops = BatchOperations()

# Batch set operations
data = {
    'user:1000:name': 'John Doe',
    'user:1000:email': 'john@example.com',
    'user:1001:name': 'Jane Smith',
    'user:1001:email': 'jane@example.com'
}
results = batch_ops.batch_set(data)
print(f"Set operations: {results}")

# Batch get operations
keys = ['user:1000:name', 'user:1000:email', 'user:1001:name']
values = batch_ops.batch_get(keys)
print(f"Retrieved values: {values}")

# Batch list operations
list_ops = [
    ('lpush', 'queue:tasks', 'task1', 'task2'),
    ('rpush', 'queue:logs', 'log1'),
    ('lrange', 'queue:tasks', 0, -1)
]
results = batch_ops.batch_list_operations(list_ops)
print(f"List operations: {results}")

# Batch hash operations
hash_ops = [
    ('hset', 'profile', 'John Doe'),
    ('hset', 'age', '30'),
    ('hincrby', 'login_count', 1),
    ('hget', 'profile'),
    ('hget', 'login_count')
]
results = batch_ops.batch_hash_operations('user:1000', hash_ops)
print(f"Hash operations: {results}")</code></pre>

            <h3>Advanced Pipelining Patterns</h3>
            
            <h4>Chunked Processing</h4>
            <pre><code>import redis
from typing import List, Iterator, Any

def chunked_pipeline_processing(redis_client: redis.Redis, 
                               operations: List[tuple], 
                               chunk_size: int = 1000) -> List[Any]:
    """Process large number of operations in chunks"""
    
    def chunks(lst: List, n: int) -> Iterator[List]:
        """Yield successive n-sized chunks from lst"""
        for i in range(0, len(lst), n):
            yield lst[i:i + n]
    
    all_results = []
    
    for chunk in chunks(operations, chunk_size):
        pipe = redis_client.pipeline()
        
        for operation in chunk:
            command, *args = operation
            getattr(pipe, command)(*args)
        
        chunk_results = pipe.execute()
        all_results.extend(chunk_results)
    
    return all_results

# Example: Process 10,000 SET operations in chunks
redis_client = redis.Redis(decode_responses=True)

# Generate large number of operations
operations = [('set', f'key:{i}', f'value:{i}') for i in range(10000)]

# Process in chunks of 1000
results = chunked_pipeline_processing(redis_client, operations, chunk_size=1000)
print(f"Processed {len(results)} operations")</code></pre>

            <h4>Pipeline with Error Handling</h4>
            <pre><code>import redis
from typing import List, Tuple, Any

class RobustPipeline:
    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client
    
    def execute_with_error_handling(self, operations: List[tuple]) -> List[Tuple[bool, Any]]:
        """Execute operations and return (success, result) tuples"""
        pipe = self.redis_client.pipeline()
        
        # Add all operations to pipeline
        for operation in operations:
            command, *args = operation
            try:
                getattr(pipe, command)(*args)
            except AttributeError:
                # Invalid command, add a dummy operation
                pipe.ping()
        
        try:
            results = pipe.execute()
            
            # Process results
            processed_results = []
            for i, (operation, result) in enumerate(zip(operations, results)):
                command = operation[0]
                
                if isinstance(result, redis.ResponseError):
                    processed_results.append((False, str(result)))
                else:
                    processed_results.append((True, result))
            
            return processed_results
            
        except redis.RedisError as e:
            # Return error for all operations
            return [(False, str(e)) for _ in operations]
    
    def execute_with_fallback(self, operations: List[tuple], 
                            fallback_individual: bool = True) -> List[Tuple[bool, Any]]:
        """Execute with fallback to individual operations on pipeline failure"""
        try:
            return self.execute_with_error_handling(operations)
        except Exception as e:
            if not fallback_individual:
                return [(False, str(e)) for _ in operations]
            
            # Fallback to individual operations
            print(f"Pipeline failed, falling back to individual operations: {e}")
            results = []
            
            for operation in operations:
                command, *args = operation
                try:
                    result = getattr(self.redis_client, command)(*args)
                    results.append((True, result))
                except Exception as op_error:
                    results.append((False, str(op_error)))
            
            return results

# Usage example
redis_client = redis.Redis(decode_responses=True)
robust_pipe = RobustPipeline(redis_client)

operations = [
    ('set', 'key1', 'value1'),
    ('incr', 'counter'),
    ('get', 'key1'),
    ('invalid_command', 'arg'),  # This will fail
    ('lpush', 'list1', 'item1')
]

results = robust_pipe.execute_with_error_handling(operations)

for i, (success, result) in enumerate(results):
    operation = operations[i]
    if success:
        print(f"✓ {operation[0]}: {result}")
    else:
        print(f"✗ {operation[0]}: {result}")</code></pre>
        </section>

        <section>
            <h2>Combining Transactions and Pipelining</h2>
            
            <h3>Transactional Pipelining</h3>
            <p>Combine the atomic guarantees of transactions with the performance benefits of pipelining:</p>
            
            <h4>Batch Transaction Processing</h4>
            <pre><code>import redis
from typing import List, Dict, Any

class TransactionalBatch:
    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client
    
    def execute_batch_transactions(self, transaction_groups: List[List[tuple]]) -> List[bool]:
        """Execute multiple independent transactions in a batch"""
        pipe = self.redis_client.pipeline()
        transaction_boundaries = []
        
        for group in transaction_groups:
            start_pos = len(transaction_boundaries)
            
            # Start transaction
            pipe.multi()
            
            # Add operations
            for operation in group:
                command, *args = operation
                getattr(pipe, command)(*args)
            
            # Mark transaction end
            transaction_boundaries.append((start_pos, len(group)))
        
        try:
            # Execute all transactions
            results = pipe.execute()
            
            # Process results per transaction
            transaction_results = []
            result_index = 0
            
            for start_pos, op_count in transaction_boundaries:
                # Each transaction returns a list of results or None (if failed)
                transaction_result = results[result_index]
                transaction_results.append(transaction_result is not None)
                result_index += 1
            
            return transaction_results
            
        except redis.RedisError:
            return [False] * len(transaction_groups)
    
    def conditional_batch_operations(self, watched_keys: List[str], 
                                   operations: List[tuple]) -> bool:
        """Execute batch operations only if watched keys unchanged"""
        try:
            # Watch all keys
            self.redis_client.watch(*watched_keys)
            
            # Use pipeline for transaction
            pipe = self.redis_client.pipeline()
            pipe.multi()
            
            # Add all operations
            for operation in operations:
                command, *args = operation
                getattr(pipe, command)(*args)
            
            # Execute transaction
            result = pipe.execute()
            return result is not None
            
        except redis.WatchError:
            return False

# Example: User profile update with inventory check
class UserProfileService:
    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client
        self.batch_processor = TransactionalBatch(redis_client)
    
    def update_user_with_purchase(self, user_id: str, item_id: str, 
                                price: float, quantity: int) -> bool:
        """Update user profile and inventory atomically"""
        
        user_balance_key = f"user:{user_id}:balance"
        inventory_key = f"inventory:{item_id}"
        user_items_key = f"user:{user_id}:items"
        
        # Watch keys that might change
        watched_keys = [user_balance_key, inventory_key]
        
        # Get current values
        current_balance = self.redis_client.get(user_balance_key)
        current_inventory = self.redis_client.get(inventory_key)
        
        if not current_balance or not current_inventory:
            return False
        
        current_balance = float(current_balance)
        current_inventory = int(current_inventory)
        total_cost = price * quantity
        
        # Check constraints
        if current_balance < total_cost or current_inventory < quantity:
            return False
        
        # Define operations
        operations = [
            ('set', user_balance_key, current_balance - total_cost),
            ('set', inventory_key, current_inventory - quantity),
            ('hincrby', user_items_key, item_id, quantity),
            ('lpush', f"user:{user_id}:purchase_history", 
             f"{item_id}:{quantity}:{price}:{time.time()}")
        ]
        
        return self.batch_processor.conditional_batch_operations(watched_keys, operations)

# Usage
redis_client = redis.Redis(decode_responses=True)
profile_service = UserProfileService(redis_client)

# Setup test data
redis_client.set("user:1000:balance", 1000.0)
redis_client.set("inventory:sword", 5)

# Attempt purchase
success = profile_service.update_user_with_purchase(
    user_id="1000",
    item_id="sword", 
    price=299.99,
    quantity=1
)

print(f"Purchase successful: {success}")
if success:
    print(f"New balance: {redis_client.get('user:1000:balance')}")
    print(f"Remaining inventory: {redis_client.get('inventory:sword')}")</code></pre>

            <h3>Pipeline with Conditional Logic</h3>
            
            <h4>Multi-Stage Processing</h4>
            <pre><code>import redis
from typing import List, Dict, Any, Optional

class ConditionalPipeline:
    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client
    
    def process_user_actions(self, user_actions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Process multiple user actions with conditional logic"""
        
        results = {
            'successful_actions': [],
            'failed_actions': [],
            'total_processed': 0,
            'total_failed': 0
        }
        
        for action in user_actions:
            action_type = action.get('type')
            user_id = action.get('user_id')
            
            try:
                if action_type == 'login':
                    success = self._process_login(user_id, action)
                elif action_type == 'purchase':
                    success = self._process_purchase(user_id, action)
                elif action_type == 'update_profile':
                    success = self._process_profile_update(user_id, action)
                else:
                    success = False
                
                if success:
                    results['successful_actions'].append(action)
                    results['total_processed'] += 1
                else:
                    results['failed_actions'].append(action)
                    results['total_failed'] += 1
                    
            except Exception as e:
                action['error'] = str(e)
                results['failed_actions'].append(action)
                results['total_failed'] += 1
        
        return results
    
    def _process_login(self, user_id: str, action: Dict[str, Any]) -> bool:
        """Process login action with multiple updates"""
        try:
            # Watch user state
            user_key = f"user:{user_id}"
            self.redis_client.watch(user_key)
            
            # Check if user exists and is active
            user_status = self.redis_client.hget(user_key, 'status')
            if user_status != 'active':
                self.redis_client.unwatch()
                return False
            
            # Perform login updates
            pipe = self.redis_client.pipeline()
            pipe.multi()
            
            # Update last login
            pipe.hset(user_key, 'last_login', time.time())
            
            # Increment login count
            pipe.hincrby(user_key, 'login_count', 1)
            
            # Add to active sessions
            session_id = action.get('session_id', 'unknown')
            pipe.sadd(f"active_sessions:{user_id}", session_id)
            
            # Update daily login stats
            today = time.strftime('%Y-%m-%d')
            pipe.incr(f"daily_logins:{today}")
            
            # Log activity
            pipe.lpush(f"user:{user_id}:activity", f"login:{time.time()}")
            pipe.ltrim(f"user:{user_id}:activity", 0, 99)  # Keep last 100 activities
            
            result = pipe.execute()
            return result is not None
            
        except redis.WatchError:
            return False
    
    def _process_purchase(self, user_id: str, action: Dict[str, Any]) -> bool:
        """Process purchase with inventory and balance checks"""
        item_id = action.get('item_id')
        quantity = action.get('quantity', 1)
        
        if not item_id:
            return False
        
        try:
            # Watch relevant keys
            balance_key = f"user:{user_id}:balance"
            inventory_key = f"inventory:{item_id}"
            
            self.redis_client.watch(balance_key, inventory_key)
            
            # Get current values
            current_balance = self.redis_client.get(balance_key)
            current_inventory = self.redis_client.get(inventory_key)
            item_price = self.redis_client.hget(f"items:{item_id}", 'price')
            
            if not all([current_balance, current_inventory, item_price]):
                self.redis_client.unwatch()
                return False
            
            current_balance = float(current_balance)
            current_inventory = int(current_inventory)
            item_price = float(item_price)
            total_cost = item_price * quantity
            
            # Check constraints
            if current_balance < total_cost or current_inventory < quantity:
                self.redis_client.unwatch()
                return False
            
            # Process purchase
            pipe = self.redis_client.pipeline()
            pipe.multi()
            
            # Update balance and inventory
            pipe.set(balance_key, current_balance - total_cost)
            pipe.set(inventory_key, current_inventory - quantity)
            
            # Add to user inventory
            pipe.hincrby(f"user:{user_id}:inventory", item_id, quantity)
            
            # Record purchase
            purchase_record = f"{item_id}:{quantity}:{item_price}:{time.time()}"
            pipe.lpush(f"user:{user_id}:purchases", purchase_record)
            
            # Update sales stats
            pipe.hincrby("sales_stats", item_id, quantity)
            pipe.incrbyfloat("revenue_stats", "total", total_cost)
            
            result = pipe.execute()
            return result is not None
            
        except redis.WatchError:
            return False
    
    def _process_profile_update(self, user_id: str, action: Dict[str, Any]) -> bool:
        """Process profile update"""
        updates = action.get('updates', {})
        
        if not updates:
            return False
        
        try:
            user_key = f"user:{user_id}"
            
            pipe = self.redis_client.pipeline()
            
            # Update profile fields
            for field, value in updates.items():
                if field in ['name', 'email', 'bio', 'location']:  # Allowed fields
                    pipe.hset(user_key, field, value)
            
            # Update last modified timestamp
            pipe.hset(user_key, 'last_modified', time.time())
            
            # Log profile update
            pipe.lpush(f"user:{user_id}:activity", f"profile_update:{time.time()}")
            pipe.ltrim(f"user:{user_id}:activity", 0, 99)
            
            results = pipe.execute()
            return all(results)
            
        except redis.RedisError:
            return False

# Usage example
redis_client = redis.Redis(decode_responses=True)
conditional_pipeline = ConditionalPipeline(redis_client)

# Setup test data
redis_client.hset("user:1000", "status", "active", "balance", "1000", "login_count", "0")
redis_client.set("inventory:sword", "5")
redis_client.hset("items:sword", "price", "299.99")

# Process multiple actions
user_actions = [
    {
        'type': 'login',
        'user_id': '1000',
        'session_id': 'sess_123',
        'ip': '192.168.1.100'
    },
    {
        'type': 'purchase',
        'user_id': '1000',
        'item_id': 'sword',
        'quantity': 1
    },
    {
        'type': 'update_profile',
        'user_id': '1000',
        'updates': {
            'name': 'John Doe',
            'location': 'New York'
        }
    }
]

results = conditional_pipeline.process_user_actions(user_actions)
print(f"Processed: {results['total_processed']}, Failed: {results['total_failed']}")
print(f"Successful actions: {len(results['successful_actions'])}")
print(f"Failed actions: {len(results['failed_actions'])}")</code></pre>
        </section>

        <section>
            <h2>Performance Optimization and Best Practices</h2>
            
            <h3>Transaction Best Practices</h3>
            
            <h4>Keep Transactions Small</h4>
            <pre><code># Good: Small, focused transaction
MULTI
SET user:1000:status "active"
INCR user:1000:login_count
EXEC

# Bad: Large transaction that blocks other operations
MULTI
# ... 100+ commands ...
EXEC</code></pre>

            <h4>Minimize WATCH Usage</h4>
            <pre><code># Good: Watch only what you need
WATCH user:1000:balance
current_balance = GET user:1000:balance
MULTI
SET user:1000:balance new_balance
EXEC

# Bad: Watching too many keys increases conflict probability
WATCH user:1000:balance user:1000:status user:1000:preferences
# ... transaction logic ...</code></pre>

            <h3>Pipelining Optimization</h3>
            
            <h4>Optimal Batch Sizes</h4>
            <pre><code>import redis
import time

def find_optimal_batch_size(redis_client: redis.Redis, max_size: int = 10000):
    """Find optimal pipeline batch size for your setup"""
    
    batch_sizes = [100, 500, 1000, 2000, 5000, 10000]
    results = {}
    
    for batch_size in batch_sizes:
        if batch_size > max_size:
            continue
        
        # Test pipeline performance
        start_time = time.time()
        
        pipe = redis_client.pipeline()
        for i in range(batch_size):
            pipe.set(f"test:{i}", f"value:{i}")
        
        pipe.execute()
        
        end_time = time.time()
        duration = end_time - start_time
        ops_per_second = batch_size / duration
        
        results[batch_size] = {
            'duration': duration,
            'ops_per_second': ops_per_second
        }
        
        print(f"Batch size {batch_size}: {ops_per_second:.0f} ops/sec")
        
        # Cleanup
        pipe = redis_client.pipeline()
        for i in range(batch_size):
            pipe.delete(f"test:{i}")
        pipe.execute()
    
    # Find optimal batch size
    optimal = max(results.items(), key=lambda x: x[1]['ops_per_second'])
    print(f"Optimal batch size: {optimal[0]} ({optimal[1]['ops_per_second']:.0f} ops/sec)")
    
    return optimal[0]

# Find optimal size for your environment
redis_client = redis.Redis(decode_responses=True)
optimal_batch_size = find_optimal_batch_size(redis_client)</code></pre>

            <h4>Memory-Aware Pipelining</h4>
            <pre><code>import redis
import psutil
from typing import List, Any

class MemoryAwarePipeline:
    def __init__(self, redis_client: redis.Redis, max_memory_mb: int = 100):
        self.redis_client = redis_client
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self.current_pipeline = redis_client.pipeline()
        self.current_size = 0
        self.results = []
    
    def add_operation(self, command: str, *args) -> None:
        """Add operation to pipeline, executing if memory limit reached"""
        
        # Estimate memory usage (rough approximation)
        estimated_size = self._estimate_operation_size(command, *args)
        
        if self.current_size + estimated_size > self.max_memory_bytes:
            # Execute current pipeline and start new one
            self._execute_current_pipeline()
            self._reset_pipeline()
        
        # Add operation to current pipeline
        getattr(self.current_pipeline, command)(*args)
        self.current_size += estimated_size
    
    def execute(self) -> List[Any]:
        """Execute remaining operations and return all results"""
        if self.current_size > 0:
            self._execute_current_pipeline()
        
        return self.results
    
    def _estimate_operation_size(self, command: str, *args) -> int:
        """Rough estimation of memory usage for operation"""
        # Basic estimation based on string lengths
        size = len(command) + 50  # Command overhead
        
        for arg in args:
            if isinstance(arg, str):
                size += len(arg.encode('utf-8'))
            elif isinstance(arg, (int, float)):
                size += 8
            else:
                size += len(str(arg).encode('utf-8'))
        
        return size
    
    def _execute_current_pipeline(self) -> None:
        """Execute current pipeline and store results"""
        if self.current_size > 0:
            try:
                batch_results = self.current_pipeline.execute()
                self.results.extend(batch_results)
            except redis.RedisError as e:
                print(f"Pipeline execution error: {e}")
    
    def _reset_pipeline(self) -> None:
        """Reset pipeline state"""
        self.current_pipeline = self.redis_client.pipeline()
        self.current_size = 0

# Usage example
redis_client = redis.Redis(decode_responses=True)
memory_pipeline = MemoryAwarePipeline(redis_client, max_memory_mb=50)

# Add many operations
for i in range(10000):
    memory_pipeline.add_operation('set', f'key:{i}', f'value:{i}' * 100)  # Large values

# Execute all
results = memory_pipeline.execute()
print(f"Executed {len(results)} operations")</code></pre>

            <h3>Monitoring and Metrics</h3>
            
            <h4>Transaction Monitoring</h4>
            <pre><code>#!/bin/bash
# Monitor Redis transaction performance

REDIS_CLI="redis-cli"

echo "=== Redis Transaction Monitoring ==="

# Get basic stats
echo "Connected clients:"
$REDIS_CLI INFO clients | grep connected_clients

echo "Commands processed:"
$REDIS_CLI INFO stats | grep total_commands_processed

echo "Keyspace operations:"
$REDIS_CLI INFO stats | grep keyspace

# Monitor for MULTI/EXEC commands
echo "Recent MULTI/EXEC activity:"
$REDIS_CLI MONITOR | grep -E "(MULTI|EXEC|WATCH|UNWATCH)" | head -20

# Check for long-running operations
echo "Slowlog entries:"
$REDIS_CLI SLOWLOG GET 10</code></pre>

            <h4>Performance Metrics Collection</h4>
            <pre><code>import redis
import time
import statistics
from typing import List, Dict, Any

class RedisPerformanceMonitor:
    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client
        self.metrics = {
            'transaction_times': [],
            'pipeline_times': [],
            'individual_command_times': [],
            'transaction_success_rate': 0,
            'pipeline_success_rate': 0
        }
    
    def benchmark_transaction(self, operations: List[tuple], iterations: int = 100) -> Dict[str, float]:
        """Benchmark transaction performance"""
        times = []
        successes = 0
        
        for _ in range(iterations):
            start_time = time.time()
            
            try:
                pipe = self.redis_client.pipeline()
                pipe.multi()
                
                for operation in operations:
                    command, *args = operation
                    getattr(pipe, command)(*args)
                
                result = pipe.execute()
                
                if result is not None:
                    successes += 1
                
                end_time = time.time()
                times.append(end_time - start_time)
                
            except Exception as e:
                end_time = time.time()
                times.append(end_time - start_time)
        
        self.metrics['transaction_times'].extend(times)
        self.metrics['transaction_success_rate'] = successes / iterations
        
        return {
            'avg_time': statistics.mean(times),
            'median_time': statistics.median(times),
            'min_time': min(times),
            'max_time': max(times),
            'success_rate': successes / iterations
        }
    
    def benchmark_pipeline(self, operations: List[tuple], iterations: int = 100) -> Dict[str, float]:
        """Benchmark pipeline performance"""
        times = []
        successes = 0
        
        for _ in range(iterations):
            start_time = time.time()
            
            try:
                pipe = self.redis_client.pipeline()
                
                for operation in operations:
                    command, *args = operation
                    getattr(pipe, command)(*args)
                
                results = pipe.execute()
                successes += 1
                
                end_time = time.time()
                times.append(end_time - start_time)
                
            except Exception as e:
                end_time = time.time()
                times.append(end_time - start_time)
        
        self.metrics['pipeline_times'].extend(times)
        self.metrics['pipeline_success_rate'] = successes / iterations
        
        return {
            'avg_time': statistics.mean(times),
            'median_time': statistics.median(times),
            'min_time': min(times),
            'max_time': max(times),
            'success_rate': successes / iterations
        }
    
    def benchmark_individual_commands(self, operations: List[tuple], iterations: int = 100) -> Dict[str, float]:
        """Benchmark individual command performance"""
        times = []
        
        for _ in range(iterations):
            start_time = time.time()
            
            for operation in operations:
                command, *args = operation
                try:
                    getattr(self.redis_client, command)(*args)
                except Exception:
                    pass
            
            end_time = time.time()
            times.append(end_time - start_time)
        
        self.metrics['individual_command_times'].extend(times)
        
        return {
            'avg_time': statistics.mean(times),
            'median_time': statistics.median(times),
            'min_time': min(times),
            'max_time': max(times)
        }
    
    def generate_report(self) -> str:
        """Generate performance report"""
        report = "=== Redis Performance Report ===\n\n"
        
        if self.metrics['transaction_times']:
            trans_times = self.metrics['transaction_times']
            report += f"Transaction Performance:\n"
            report += f"  Average: {statistics.mean(trans_times):.4f}s\n"
            report += f"  Median: {statistics.median(trans_times):.4f}s\n"
            report += f"  Success Rate: {self.metrics['transaction_success_rate']:.2%}\n\n"
        
        if self.metrics['pipeline_times']:
            pipe_times = self.metrics['pipeline_times']
            report += f"Pipeline Performance:\n"
            report += f"  Average: {statistics.mean(pipe_times):.4f}s\n"
            report += f"  Median: {statistics.median(pipe_times):.4f}s\n"
            report += f"  Success Rate: {self.metrics['pipeline_success_rate']:.2%}\n\n"
        
        if self.metrics['individual_command_times']:
            cmd_times = self.metrics['individual_command_times']
            report += f"Individual Commands Performance:\n"
            report += f"  Average: {statistics.mean(cmd_times):.4f}s\n"
            report += f"  Median: {statistics.median(cmd_times):.4f}s\n\n"
        
        # Calculate speedup
        if (self.metrics['pipeline_times'] and 
            self.metrics['individual_command_times']):
            pipe_avg = statistics.mean(self.metrics['pipeline_times'])
            cmd_avg = statistics.mean(self.metrics['individual_command_times'])
            speedup = cmd_avg / pipe_avg
            report += f"Pipeline Speedup: {speedup:.1f}x\n"
        
        return report

# Usage example
redis_client = redis.Redis(decode_responses=True)
monitor = RedisPerformanceMonitor(redis_client)

# Define test operations
operations = [
    ('set', 'key1', 'value1'),
    ('incr', 'counter'),
    ('lpush', 'list1', 'item1'),
    ('hset', 'hash1', 'field1', 'value1'),
    ('sadd', 'set1', 'member1')
]

# Benchmark different approaches
print("Benchmarking transaction performance...")
trans_results = monitor.benchmark_transaction(operations)

print("Benchmarking pipeline performance...")
pipe_results = monitor.benchmark_pipeline(operations)

print("Benchmarking individual commands...")
cmd_results = monitor.benchmark_individual_commands(operations)

# Generate report
print(monitor.generate_report())</code></pre>
        </section>

        <section>
            <h2>Next Steps</h2>
            <p>With Redis transactions and pipelining mastered, you can now build applications that maintain data consistency while achieving optimal performance. In the next posts of this series, we'll explore:</p>
            <ul>
                <li><strong>Server-side Programming with Lua Scripts:</strong> Advanced operations and custom functionality for complex atomic operations</li>
                <li><strong>Extending Redis with Modules:</strong> RedisJSON, RedisSearch, and other powerful extensions</li>
                <li><strong>Redis Monitoring and Performance Optimization:</strong> Advanced monitoring techniques and optimization strategies</li>
            </ul>
        </section>

        <section>
            <h2>Key Takeaways</h2>
            <ul>
                <li>Redis transactions provide atomicity but not traditional ACID guarantees</li>
                <li>Use MULTI/EXEC for simple atomic operations, WATCH for optimistic concurrency control</li>
                <li>Pipelining dramatically improves performance by reducing network round-trips</li>
                <li>Combine transactions and pipelining for both consistency and performance</li>
                <li>Keep transactions small to minimize blocking and conflict probability</li>
                <li>Use appropriate batch sizes in pipelines to balance memory usage and performance</li>
                <li>Implement proper error handling for both individual and batch operations</li>
                <li>Monitor transaction success rates and pipeline performance metrics</li>
                <li>Consider using Lua scripts for complex atomic operations beyond simple transactions</li>
                <li>Test performance characteristics in your specific environment and use case</li>
            </ul>
        </section>

        <section>
            <h2>Practice Exercises</h2>
            <ol>
                <li>Implement a banking system with atomic transfers using transactions</li>
                <li>Build a shopping cart system with optimistic concurrency control</li>
                <li>Create a distributed locking mechanism using WATCH/UNWATCH</li>
                <li>Optimize batch operations using pipelining for your specific workload</li>
                <li>Implement retry logic for failed transactions due to key conflicts</li>
                <li>Build a performance monitoring system for Redis operations</li>
                <li>Create a queue system that combines transactions and pipelining</li>
                <li>Implement conditional operations based on current data state</li>
            </ol>
        </section>

        <section>
            <h2>Further Reading</h2>
            <ul>
                <li><a href="https://redis.io/topics/transactions">Official Redis Transactions Documentation</a></li>
                <li><a href="https://redis.io/topics/pipelining">Redis Pipelining Guide</a></li>
                <li><a href="https://redis.io/commands#transactions">Transaction Commands Reference</a></li>
                <li><a href="https://redis.io/topics/distlock">Distributed Locking with Redis</a></li>
                <li><a href="https://redis.io/topics/benchmarks">Redis Benchmarking Guide</a></li>
                <li><a href="https://redis.io/topics/clients">Redis Client Libraries</a></li>
            </ul>
        </section>
    </main>

    <footer>
        <hr>
        <p><em>This is the eighth post in our comprehensive Redis series. You now have the knowledge to implement atomic operations and high-performance batch processing with Redis.</em></p>
        <p><strong>Previous:</strong> <a href="07-real-time-messaging-redis-pubsub.html">Real-time Messaging with Redis Pub/Sub</a></p>
        <p><strong>Next up:</strong> Server-side Programming with Lua Scripts</p>
    </footer>
</body>
</html>
